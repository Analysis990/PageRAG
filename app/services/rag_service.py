import os
from openai import OpenAI
from qdrant_client import QdrantClient
from dotenv import load_dotenv

# Load env variables (normally handled by main's load_dotenv, but good to be safe)
load_dotenv()

QDRANT_URL = os.getenv("QDRANT_URL", "http://localhost:6333")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL")  # None = 使用 OpenAI 官方端點
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")  # 預設使用 gpt-3.5-turbo

async def query(message: str):
    """
    Query the RAG knowledge base using a simplified direct OpenAI approach.
    This avoids the 'langchain.chains' module error until the environment can be fully fixed.
    """
    if not OPENAI_API_KEY:
        return "錯誤：找不到 OPENAI_API_KEY。", []

    try:
        # 1. Initialize OpenAI Client
        client_kwargs = {"api_key": OPENAI_API_KEY}
        if OPENAI_BASE_URL:
            client_kwargs["base_url"] = OPENAI_BASE_URL
        openai_client = OpenAI(**client_kwargs)

        # 2. Check if Qdrant collection exists
        qdrant_client = QdrantClient(url=QDRANT_URL)
        
        collection_exists = False
        try:
            collections = qdrant_client.get_collections()
            collection_exists = any(c.name == "rag_documents" for c in collections.collections)
        except Exception as qe:
            print(f"Qdrant Check Error: {qe}")

        # 3. If no collection, use direct LLM
        if not collection_exists:
            print("No RAG data found, using direct LLM response...")
            response = openai_client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=[{"role": "user", "content": message}],
                temperature=0.7
            )
            return response.choices[0].message.content, ["Direct LLM (No RAG data)"]

        # 4. Perform Search (Vector Retrieval)
        # We need embeddings for the query
        # Using a simpler manual retrieval approach to avoid LangChain dependencies for now
        # Note: This assumes embeddings are generated by process_rag.py
        
        # We'll use the OpenAI client to get query embeddings
        embeddings_response = openai_client.embeddings.create(
            input=[message],
            model="text-embedding-3-small" # Default model for LangChain's OpenAIEmbeddings
        )
        query_vector = embeddings_response.data[0].embedding
        
        # Search in Qdrant
        search_results = qdrant_client.search(
            collection_name="rag_documents",
            query_vector=query_vector,
            limit=3
        )
        
        if not search_results:
            # Fallback to direct LLM if no relevant chunks found
            response = openai_client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=[{"role": "user", "content": message}],
                temperature=0.7
            )
            return response.choices[0].message.content, ["Direct LLM (Empty search results)"]
            
        # 5. Build Context and Generate Answer
        context = "\n\n".join([res.payload.get("page_content", "") for res in search_results])
        sources = list(set([res.payload.get("metadata", {}).get("source", "Unknown") for res in search_results]))
        
        system_prompt = (
            "你是一個專門負責回答問題的助理。請根據以下提供的檢索片段來回答使用者的問題。\n"
            "1. 必須使用「繁體中文」回答。\n"
            "2. 如果片段中包含具體數據或金額，請務必列出。\n"
            "3. 如果你從提供的內容中找不到答案，請老實說「我不知道」。\n"
            f"\n\n參考內容：\n{context}"
        )
        
        final_response = openai_client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": message}
            ],
            temperature=0
        )
        
        return final_response.choices[0].message.content, sources

    except Exception as e:
        print(f"RAG Error: {e}")
        import traceback
        traceback.print_exc()
        return f"Error processing request: {str(e)}", []
